{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b257de",
   "metadata": {},
   "source": [
    "# üõ°Ô∏èProyecto SafeStep: Asistente AR para Visi√≥n Reducida\n",
    "\n",
    "Para ver el proyecto en su totalidad y no solo el notebook con la implementaci√≥n y ejecuci√≥n de este, puede acceder al siguiente repositorio en Github, que contiene los v√≠deos de entrada y salida, los audios con las √≥rdenes y los generados por el asistente, as√≠ como las im√°genes tambi√©n de entrada y salida utilizadas. Adem√°s, dispone de un README.md que explica con detalle el flujo de trabajo llevado a cabo para la realizaci√≥n del SafeStep:\n",
    "\n",
    "https://github.com/aos35/multimodal.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de2a084",
   "metadata": {},
   "source": [
    "**SafeStep** es un sistema de asistencia inteligente dise√±ado para aumentar la autonom√≠a y seguridad de peatones con visi√≥n reducida en entornos urbanos. Utilizando t√©cnicas avanzadas de **Visi√≥n Artificial y Realidad Aumentada (AR)**, el sistema procesa el entorno en tiempo real para identificar peligros, interpretar se√±ales y ofrecer feedback multimodal (visual, auditivo y h√°ptico)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141d578",
   "metadata": {},
   "source": [
    "### üéØObjetivos Principales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7d076",
   "metadata": {},
   "source": [
    "El sistema busca resolver la dificultad de detectar elementos cr√≠ticos en el tr√°fico (como el estado de un sem√°foro lejano o un coche silencioso acerc√°ndose) mediante:\n",
    "\n",
    "1. **Detecci√≥n de Peligros Din√°micos:** Monitorizaci√≥n constante de veh√≠culos y peatones con alertas de colisi√≥n.\n",
    "\n",
    "2. **Mejora Visual (Smart Zoom):** Una \"Lupa Inteligente\" que detecta, recorta y ampl√≠a autom√°ticamente sem√°foros y se√±ales lejanas.\n",
    "\n",
    "3. **Navegaci√≥n Segura:** Identificaci√≥n de pasos de cebra y estimaci√≥n de distancia a objetos.\n",
    "\n",
    "4. **Interfaz de Alto Contraste:** UI accesible con c√≥digos de color simplificados (üî¥ Peligro / üü¢ Seguro) y textos de gran tama√±o.\n",
    "\n",
    "5. **Accesibilidad Multimodal**: Generaci√≥n de avisos tanto visuales como sonoros para usuarios con baja visi√≥n o en situaciones de alta distracci√≥n.\n",
    "\n",
    "6. **Personalizaci√≥n de Alertas**: Adaptaci√≥n de la cantidad y tipo de avisos seg√∫n el contexto o preferencias del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2639cc8",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Tecnolog√≠as Implementadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a67b6",
   "metadata": {},
   "source": [
    "Este prototipo integra un pipeline complejo de procesamiento de video:\n",
    "\n",
    "- **YOLOv8 (Ultralytics):** Para la detecci√≥n y tracking (seguimiento) de objetos en tiempo real.\n",
    "\n",
    "- **OpenCV:** Para el procesamiento de imagen (CLAHE para visi√≥n nocturna, detecci√≥n de l√≠neas de carril, superposiciones gr√°ficas).\n",
    "\n",
    "- **EasyOCR:** Para el reconocimiento √≥ptico de caracteres (OCR) en se√±ales de tr√°fico y carteles.\n",
    "\n",
    "- **Whisper (OpenAI):** Para la comprensi√≥n de comandos de voz del usuario.\n",
    "\n",
    "- **gTTS (Google Text-to-Speech):** Para la generaci√≥n de avisos auditivos y lectura de se√±ales.\n",
    "\n",
    "- **Spacy:** Para el procesamiento de lenguaje natural (NLP) e interpretaci√≥n de intenciones.\n",
    "\n",
    "- **MoviePy:** Para la edici√≥n de video y sincronizaci√≥n de audio/video.\n",
    "\n",
    "El sistema combina t√©cnicas avanzadas de visi√≥n por computador, procesamiento de lenguaje y s√≠ntesis de voz para ofrecer una experiencia accesible y robusta en tiempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46502dfd",
   "metadata": {},
   "source": [
    "## 1. Preparaci√≥n del Entorno\n",
    "Instalaci√≥n de todas las dependencias necesarias para el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Instalaci√≥n de librer√≠as base (si no est√°n instaladas)\n",
    "!pip install -q openai-whisper ultralytics git+https://github.com/openai/CLIP.git\n",
    "!pip install -q moviepy gTTS spacy\n",
    "!python -m spacy download es_core_news_sm\n",
    "!pip install -q easyocr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c1530",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n y Carga de Modelos\n",
    "Inicializaci√≥n de modelos de IA (YOLO, Whisper, Spacy, EasyOCR) y configuraci√≥n de hardware (GPU/CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ CONFIGURACI√ìN Y CARGA DE MODELOS (Ejecutar solo una vez)\n",
    "# ==================================================================================\n",
    "# Este bloque inicializa todos los modelos de IA y configura el entorno.\n",
    "# Se ejecuta una sola vez al principio para evitar recargar modelos pesados en cada video.\n",
    "# ==================================================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import whisper\n",
    "import spacy\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from gtts import gTTS\n",
    "import easyocr\n",
    "from IPython.display import Audio, display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Intentar importar MoviePy (Manejo de compatibilidad entre versiones 1.x y 2.x)\n",
    "try:\n",
    "    from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip\n",
    "    IS_MOVIEPY_2 = False\n",
    "except ImportError:\n",
    "    from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "    from moviepy.audio.io.AudioFileClip import AudioFileClip\n",
    "    from moviepy.audio.AudioClip import CompositeAudioClip\n",
    "    IS_MOVIEPY_2 = True\n",
    "\n",
    "# --- CONFIGURACI√ìN DE HARDWARE ---\n",
    "# Detecta si hay una GPU NVIDIA disponible para acelerar el procesamiento (CUDA)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'‚úÖ Usando dispositivo: {device}')\n",
    "\n",
    "print(\"‚è≥ Cargando modelos (Whisper, Spacy, YOLO, EasyOCR)...\")\n",
    "\n",
    "# --- 1. MODELO DE AUDIO (Whisper) ---\n",
    "# Se usa para transcribir comandos de voz del usuario a texto.\n",
    "model_whisper = whisper.load_model('base')\n",
    "\n",
    "def transcribir_audio(ruta_audio):\n",
    "    \"\"\"Convierte un archivo de audio en texto usando OpenAI Whisper.\"\"\"\n",
    "    display(Audio(ruta_audio, autoplay=False))\n",
    "    print(f\"üéôÔ∏è Transcribiendo {ruta_audio}...\")\n",
    "    resultado = model_whisper.transcribe(ruta_audio, language=\"es\")[\"text\"]\n",
    "    print(f\"üó£Ô∏è Usuario dice: '{resultado}'\")\n",
    "    return resultado.strip()\n",
    "\n",
    "\n",
    "# --- 2. MODELO DE NLP (Spacy) ---\n",
    "# Se usa para entender la intenci√≥n del usuario (ej: \"¬øEs seguro cruzar?\" vs \"Lee el cartel\")\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Configuraci√≥n del PhraseMatcher para detectar intenciones espec√≠ficas\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LEMMA\")\n",
    "\n",
    "# A√±adir patrones para diferentes intenciones\n",
    "matcher.add(\"VEHICULOS\", [nlp(\"coche\"), nlp(\"veh√≠culo\")])\n",
    "matcher.add(\"PASO_CEBRA\", [nlp(\"paso de cebra\"), nlp(\"peat√≥n\")])\n",
    "matcher.add(\"SEMAFORO\", [nlp(\"sem√°foro\"), nlp(\"luz roja\")])\n",
    "matcher.add(\"SE√ëALES\", [nlp(\"carteles\"), nlp(\"se√±ales\")])\n",
    "matcher.add(\"SOLO_PELIGRO\", [nlp(\"peligro\"), nlp(\"cuidado\"), nlp(\"alerta\"), nlp(\"silencio\")])\n",
    "matcher.add(\"TODO\", [nlp(\"todo\"), nlp(\"general\")])\n",
    "\n",
    "# Funci√≥n para analizar la intenci√≥n del usuario\n",
    "def analizar_intencion_spacy(texto):\n",
    "    doc = nlp(texto.lower())\n",
    "    matches = matcher(doc)\n",
    "    # Retorna la primera intenci√≥n detectada\n",
    "    if matches:\n",
    "        return nlp.vocab.strings[matches[0][0]]\n",
    "    # Si no se detecta ninguna intenci√≥n, retorna \"TODO\"\n",
    "    return \"TODO\"\n",
    "\n",
    "# --- 3. MODELO DE VISI√ìN (YOLOv8) ---\n",
    "# Detecta objetos en tiempo real (coches, personas, sem√°foros, se√±ales).\n",
    "model_yolo = YOLO('yolov8n.pt')\n",
    "\n",
    "# --- 4. MODELO DE OCR (EasyOCR) ---\n",
    "# Lee texto en im√°genes. Se configura para espa√±ol e ingl√©s.\n",
    "reader = easyocr.Reader(['es', 'en'], gpu=True)\n",
    "print(\"‚úÖ Modelos cargados.\")\n",
    "\n",
    "# Clases de inter√©s de COCO dataset para tr√°fico:\n",
    "# 0: Persona, 1: Bici, 2: Coche, 3: Moto, 5: Bus, 7: Cami√≥n, 9: Sem√°foro, 11: Se√±al Stop\n",
    "CLASES_TRAFICO = [0, 1, 2, 3, 5, 7, 9, 11] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a7fde5",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuraci√≥n y Carga de Modelos\n",
    "\n",
    "Este bloque constituye la **fase de inicializaci√≥n global del sistema SafeStep** y se ejecuta **una √∫nica vez** al inicio del entorno. Su objetivo principal es **preparar todos los modelos de inteligencia artificial y dependencias necesarias**, evitando recargas innecesarias que penalizar√≠an gravemente el rendimiento durante el procesamiento de v√≠deo.\n",
    "\n",
    "En primer lugar, se importan las librer√≠as fundamentales para **visi√≥n artificial, procesamiento de lenguaje, audio y gesti√≥n de v√≠deo**, junto con utilidades auxiliares para visualizaci√≥n, control de memoria y compatibilidad entre versiones. Destaca el manejo expl√≠cito de **MoviePy 1.x y 2.x**, garantizando portabilidad del c√≥digo entre distintos entornos sin romper la ejecuci√≥n.\n",
    "\n",
    "A continuaci√≥n, el sistema detecta autom√°ticamente la disponibilidad de **aceleraci√≥n por GPU (CUDA)**, permitiendo que los modelos m√°s costosos computacionalmente (YOLO y EasyOCR) se ejecuten en GPU cuando est√° disponible, o en CPU como alternativa segura. Esta detecci√≥n din√°mica asegura un comportamiento robusto tanto en entornos locales como en notebooks o servidores.\n",
    "\n",
    "Se inicializan despu√©s los **modelos principales del pipeline**:\n",
    "- **Whisper**, encargado de transcribir comandos de voz del usuario a texto, habilitando una interacci√≥n natural y accesible.\n",
    "- **spaCy**, utilizado para el an√°lisis sem√°ntico del texto y la interpretaci√≥n de la intenci√≥n del usuario mediante un `PhraseMatcher` basado en lemas, lo que permite reconocer √≥rdenes incluso con variaciones ling√º√≠sticas.\n",
    "- **YOLOv8**, responsable de la detecci√≥n y seguimiento de objetos cr√≠ticos del entorno urbano en tiempo real.\n",
    "- **EasyOCR**, configurado para espa√±ol e ingl√©s, destinado a la lectura de texto en se√±ales y carteles.\n",
    "\n",
    "Finalmente, se definen las **clases de tr√°fico relevantes** del dataset COCO, delimitando expl√≠citamente los objetos que SafeStep considera cr√≠ticos para la seguridad peatonal (veh√≠culos, peatones, sem√°foros y se√±ales). Esta selecci√≥n reduce el ruido visual y optimiza el rendimiento del sistema, alineando la detecci√≥n con los objetivos funcionales del proyecto.\n",
    "\n",
    "En conjunto, este bloque establece una **arquitectura modular, eficiente y reutilizable**, sentando las bases t√©cnicas para el procesamiento en tiempo real sin comprometer estabilidad ni escalabilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b219ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCIONES AUXILIARES DE VISI√ìN ARTIFICIAL ---\n",
    "\n",
    "def mejorar_contraste_noche(frame):\n",
    "    \"\"\"\n",
    "    Mejora la visibilidad en condiciones de baja luz usando CLAHE (Contrast Limited Adaptive Histogram Equalization).\n",
    "    Convierte a espacio de color LAB, mejora el canal de Luminosidad (L) y reconvierte.\n",
    "    \"\"\"\n",
    "    # Convertir a LAB y aplicar CLAHE\n",
    "    lab = cv2.cvtColor(frame, cv2.COLOR_RGB2LAB)\n",
    "    # Dividir canales\n",
    "    l, a, b = cv2.split(lab)\n",
    "    # Crear objeto CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "    # Aplicar CLAHE al canal L\n",
    "    cl = clahe.apply(l)\n",
    "    # Recombinar canales y convertir de vuelta a RGB\n",
    "    limg = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "def analizar_estado_semaforo(img_roi):\n",
    "    \"\"\"\n",
    "    Determina si un sem√°foro est√° en ROJO o VERDE analizando la cantidad de p√≠xeles de color en la regi√≥n detectada.\n",
    "    Usa rangos de color en espacio HSV.\n",
    "    \"\"\"\n",
    "    # Verificar que la imagen no est√© vac√≠a\n",
    "    if img_roi.size == 0: \n",
    "        return \"DESCONOCIDO\"\n",
    "    # Convertir a espacio HSV\n",
    "    hsv = cv2.cvtColor(img_roi, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "    # Rangos para color ROJO (tiene dos rangos en HSV porque el rojo da la vuelta al c√≠rculo crom√°tico)\n",
    "    lower_red1 = np.array([0, 70, 50]); upper_red1 = np.array([10, 255, 255])\n",
    "    lower_red2 = np.array([170, 70, 50]); upper_red2 = np.array([180, 255, 255])\n",
    "    \n",
    "    # Rango para color VERDE\n",
    "    lower_green = np.array([35, 70, 50]); upper_green = np.array([90, 255, 255])\n",
    "    \n",
    "    # M√°scaras para cada color\n",
    "    mask_r1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "    mask_r2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "    mask_g = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    \n",
    "    # Contar p√≠xeles detectados\n",
    "    red_pixels = cv2.countNonZero(mask_r1) + cv2.countNonZero(mask_r2)\n",
    "    green_pixels = cv2.countNonZero(mask_g)\n",
    "    total = img_roi.shape[0] * img_roi.shape[1]\n",
    "    \n",
    "    # Umbral del 5% de p√≠xeles para considerar que el color est√° activo\n",
    "    if red_pixels > green_pixels and red_pixels > total * 0.05: return \"ROJO\"\n",
    "    elif green_pixels > red_pixels and green_pixels > total * 0.05: return \"VERDE\"\n",
    "    return \"DESCONOCIDO\"\n",
    "\n",
    "def estimar_distancia(y_bottom, h_img):\n",
    "    \"\"\"\n",
    "    Estima la distancia de un objeto bas√°ndose en la posici√≥n Y de su base en la imagen.\n",
    "    LIMITACI√ìN: Utiliza 'Flat Earth Assumption' (Suposici√≥n de Tierra Plana).\n",
    "    Asume que la c√°mara est√° a una altura fija y √°ngulo constante.\n",
    "    El horizonte est√° hardcodeado al 45% de la altura de la imagen.\n",
    "    \"\"\"\n",
    "    # Calcular horizonte\n",
    "    horizonte = h_img * 0.45 \n",
    "    if y_bottom <= horizonte: return 99.9\n",
    "    # 800 es un factor de calibraci√≥n emp√≠rico para la c√°mara usada\n",
    "    return round(800 / (y_bottom - horizonte), 1)\n",
    "\n",
    "def dibujar_paso_cebra_seguro(frame):\n",
    "    \"\"\"\n",
    "    Detecta y resalta pasos de cebra usando umbralizaci√≥n y detecci√≥n de contornos.\n",
    "    Busca patrones rectangulares blancos en la parte inferior de la imagen.\n",
    "    \"\"\"\n",
    "    # Convertir a gris y umbralizar\n",
    "    img_gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    h, w = frame.shape[:2]\n",
    "    _, mask = cv2.threshold(img_gray, 200, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Solo buscar en la mitad inferior de la imagen\n",
    "    roi_mask = np.zeros_like(mask); roi_mask[int(h*0.55):, :] = mask[int(h*0.55):, :]\n",
    "    \n",
    "    # Detectar contornos\n",
    "    contours, _ = cv2.findContours(roi_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Dibujar contornos detectados\n",
    "    overlay = frame.copy(); hay_paso = False\n",
    "    \n",
    "    # Filtrar y dibujar contornos que podr√≠an ser franjas del paso de cebra\n",
    "    for cnt in contours:\n",
    "        if 200 < cv2.contourArea(cnt) < 50000:\n",
    "            x, y, w_c, h_c = cv2.boundingRect(cnt)\n",
    "            # Filtrar por relaci√≥n de aspecto (las franjas suelen ser rectangulares)\n",
    "            if w_c / float(h_c) > 0.5:\n",
    "                cv2.drawContours(overlay, [cnt], -1, (0, 255, 0), -1); hay_paso = True\n",
    "                \n",
    "    return cv2.addWeighted(overlay, 0.4, frame, 0.6, 0) if hay_paso else frame\n",
    "\n",
    "def dibujar_radar(frame, objetos_detectados):\n",
    "    \"\"\"\n",
    "    Dibuja un minimapa tipo radar en la esquina inferior derecha.\n",
    "    Muestra la posici√≥n relativa de los objetos detectados (vista cenital).\n",
    "    \"\"\"\n",
    "    h, w = frame.shape[:2]; radar_size = 150 # Reducido para no ocupar tanta pantalla\n",
    "    radar_bg = np.zeros((radar_size, radar_size, 3), dtype=np.uint8)\n",
    "    cx = radar_size // 2; by = radar_size\n",
    "    \n",
    "    # Dibujar arcos de distancia (m√°s gruesos y visibles)\n",
    "    cv2.ellipse(radar_bg, (cx, by), (radar_size, radar_size), 0, 180, 360, (0, 150, 0), 3)\n",
    "    cv2.circle(radar_bg, (cx, by), int(radar_size * 0.33), (0, 200, 0), 2)\n",
    "    cv2.circle(radar_bg, (cx, by), int(radar_size * 0.66), (0, 200, 0), 2)\n",
    "    \n",
    "    # Dibujar al usuario (punto blanco ajustado)\n",
    "    cv2.circle(radar_bg, (cx, by - 10), 10, (255, 255, 255), -1)\n",
    "    \n",
    "    for (cls, x_c, dist) in objetos_detectados:\n",
    "        if dist > 30: continue # Solo mostrar objetos cercanos (<30m)\n",
    "        \n",
    "        # Mapear coordenadas de pantalla a coordenadas de radar\n",
    "        r_x = int((x_c / w) * radar_size)\n",
    "        r_y = max(0, min(radar_size, by - int((dist / 30.0) * radar_size)))\n",
    "        \n",
    "        # Rojo para veh√≠culos, Amarillo para otros (Puntos ajustados)\n",
    "        color = (0, 0, 255) if cls in [2, 3, 5, 7] else (0, 255, 255)\n",
    "        cv2.circle(radar_bg, (r_x, r_y), 8, color, -1)\n",
    "        \n",
    "    # Superponer el radar en el frame original\n",
    "    roi = frame[h-radar_size-10:h-10, w-radar_size-10:w-10]\n",
    "    res = cv2.addWeighted(roi, 0.5, radar_bg, 1.0, 0) # Radar totalmente opaco sobre fondo semitransparente\n",
    "    frame[h-radar_size-10:h-10, w-radar_size-10:w-10] = res\n",
    "    \n",
    "    # Marco y etiqueta (Texto ajustado)\n",
    "    cv2.rectangle(frame, (w-radar_size-10, h-radar_size-10), (w-10, h-10), (255, 255, 255), 4)\n",
    "    cv2.putText(frame, \"RADAR\", (w-radar_size, h-radar_size-15), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    return frame\n",
    "\n",
    "def ocr_carteles_images(img_path, reader, factor=5, mostrar=True):\n",
    "    # --- Cargar imagen ---\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h_img, w_img = img.shape[:2]\n",
    "\n",
    "    # --- Detectar zonas claras (cartel) ---\n",
    "    hsv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)\n",
    "    mask = cv2.inRange(hsv, (0, 0, 140), (180, 80, 255))\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 8))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    candidatos = []\n",
    "\n",
    "    for cnt in contours:\n",
    "        x, y, wc, hc = cv2.boundingRect(cnt)\n",
    "        area = wc * hc\n",
    "        aspect = wc / max(hc, 1)\n",
    "        if area > 400 and 1.5 < aspect < 12 and y < h_img * 0.65:\n",
    "            candidatos.append((x, y, wc, hc, area))\n",
    "\n",
    "    if not candidatos:\n",
    "        print(\"No se detectaron carteles.\")\n",
    "        return img_rgb\n",
    "\n",
    "    # Ordenar por √°rea\n",
    "    candidatos.sort(key=lambda c: c[4], reverse=True)\n",
    "\n",
    "    # Lista de palabras OCR global\n",
    "    palabras = []\n",
    "\n",
    "    # --- Procesar cada candidato ---\n",
    "    for c in candidatos:\n",
    "        x, y, wc, hc, _ = c\n",
    "        margin = 8\n",
    "        x1, y1 = max(0, x - margin), max(0, y - margin)\n",
    "        x2, y2 = min(w_img, x + wc + margin), min(h_img, y + hc + margin)\n",
    "        roi = img_rgb[y1:y2, x1:x2]\n",
    "        roi_big = cv2.resize(roi, None, fx=factor, fy=factor, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # OCR\n",
    "        resultados = reader.readtext(\n",
    "            roi_big,\n",
    "            paragraph=False,\n",
    "            min_size=5,\n",
    "            text_threshold=0.35,\n",
    "            low_text=0.25,\n",
    "            width_ths=0.9\n",
    "        )\n",
    "\n",
    "        # Construir lista de palabras\n",
    "        for bbox, text, prob in resultados:\n",
    "            escala_x = (x2 - x1) / roi_big.shape[1]\n",
    "            escala_y = (y2 - y1) / roi_big.shape[0]\n",
    "            bbox_scaled = [[int(px * escala_x + x1), int(py * escala_y + y1)] for px, py in bbox]\n",
    "            palabras.append({\"text\": text, \"conf\": float(prob), \"bbox\": bbox_scaled})\n",
    "\n",
    "    # --- Construir texto final ---\n",
    "    line_threshold = 25\n",
    "    lineas = []\n",
    "\n",
    "    for p in palabras:\n",
    "        y_center = sum([pt[1] for pt in p[\"bbox\"]]) / 4\n",
    "        p[\"y_center\"] = y_center\n",
    "        asignada = False\n",
    "        for l in lineas:\n",
    "            if abs(l[\"y\"] - y_center) < line_threshold:\n",
    "                l[\"items\"].append(p)\n",
    "                asignada = True\n",
    "                break\n",
    "        if not asignada:\n",
    "            lineas.append({\"y\": y_center, \"items\": [p]})\n",
    "\n",
    "    lineas.sort(key=lambda l: l[\"y\"])\n",
    "    texto_final = []\n",
    "    confs = []\n",
    "\n",
    "    for l in lineas:\n",
    "        l[\"items\"].sort(key=lambda p: sum([pt[0] for pt in p[\"bbox\"]]) / 4)\n",
    "        linea_txt = []\n",
    "        for p in l[\"items\"]:\n",
    "            palabra_mayus = p[\"text\"].upper()\n",
    "            linea_txt.append(palabra_mayus)\n",
    "            confs.append(p[\"conf\"])\n",
    "        if linea_txt:\n",
    "            texto_final.append(\" \".join(linea_txt))\n",
    "\n",
    "    texto_final_str = \" \".join(texto_final)\n",
    "    confianza_media = round(np.mean(confs), 2) if confs else 0.0\n",
    "\n",
    "    # --- Dibujar solo bounding boxes ---\n",
    "    for p in palabras:\n",
    "        x_min = min([pt[0] for pt in p[\"bbox\"]])\n",
    "        y_min = min([pt[1] for pt in p[\"bbox\"]])\n",
    "        x_max = max([pt[0] for pt in p[\"bbox\"]])\n",
    "        y_max = max([pt[1] for pt in p[\"bbox\"]])\n",
    "        cv2.rectangle(img_rgb, (x_min, y_min), (x_max, y_max), (0, 255, 0), 3)\n",
    "\n",
    "    # --- Dibujar texto final en banda inferior ---\n",
    "    alto_banda = int(h_img * 0.18)\n",
    "    overlay = img_rgb.copy()\n",
    "    cv2.rectangle(overlay, (0, h_img - alto_banda), (w_img, h_img), (0, 0, 0), -1)\n",
    "    escala = 2.5\n",
    "    grosor = 4\n",
    "    (tw, th), _ = cv2.getTextSize(texto_final_str, cv2.FONT_HERSHEY_DUPLEX, escala, grosor)\n",
    "    while tw > w_img - 20 and escala > 0.5:\n",
    "        escala -= 0.1\n",
    "        (tw, th), _ = cv2.getTextSize(texto_final_str, cv2.FONT_HERSHEY_DUPLEX, escala, grosor)\n",
    "    x_text = max(10, (w_img - tw) // 2)\n",
    "    y_text = h_img - (alto_banda // 2) + (th // 2)\n",
    "    cv2.putText(overlay, texto_final_str, (x_text, y_text),\n",
    "                cv2.FONT_HERSHEY_DUPLEX, escala, (255, 255, 0), grosor, cv2.LINE_AA)\n",
    "\n",
    "    # --- Imprimir confianzas solo por consola ---\n",
    "    print(\"\\n--- OCR PALABRA A PALABRA ---\")\n",
    "    for p in palabras:\n",
    "        print(f\"{p['text']:<20} conf={p['conf']:.2f}\")\n",
    "    print(\"\\n--- OCR FINAL ---\")\n",
    "    print(f\"Texto: {texto_final_str}\")\n",
    "    print(f\"Confianza media: {confianza_media}\")\n",
    "\n",
    "    if mostrar:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(overlay)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    return overlay\n",
    "\n",
    "# --- GENERACI√ìN DE AUDIOS PREGRABADOS ---\n",
    "# Generamos archivos de audio para las alertas comunes para no usar TTS en tiempo real (latencia).\n",
    "if not os.path.exists(\"audios\"): \n",
    "    os.makedirs(\"audios\")\n",
    "avisos = {\n",
    "    \"PELIGRO_MOVIMIENTO\": \"Peligro, veh√≠culo en movimiento detectado. No cruce.\",\n",
    "    \"COCHE_QUIETO\": \"Veh√≠culo detenido detectado.\",\n",
    "    \"PEATONES\": \"Precauci√≥n, hay peatones cerca.\",\n",
    "    \"SEMAFORO_ROJO\": \"Sem√°foro en rojo. Espere.\",\n",
    "    \"SEMAFORO_VERDE\": \"Sem√°foro en verde. Puede cruzar con precauci√≥n.\",\n",
    "    \"SE√ëAL\": \"Se√±al de tr√°fico detectada.\"\n",
    "}\n",
    "for key, texto in avisos.items():\n",
    "    f = f\"audios/aviso_{key}.mp3\"\n",
    "    if not os.path.exists(f): gTTS(texto, lang='es').save(f)\n",
    "    \n",
    "print(\"‚úÖ Configuraci√≥n completada. Modelos y audios listos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae88b94b",
   "metadata": {},
   "source": [
    "## üß† Funciones Auxiliares de Visi√≥n Artificial\n",
    "\n",
    "Este bloque agrupa funciones complementarias destinadas a **mejorar la interpretaci√≥n visual del entorno**, aportar contexto sem√°ntico y reforzar la toma de decisiones del sistema SafeStep de forma eficiente y en tiempo real.\n",
    "\n",
    "- ### üåô Mejora de visibilidad en baja iluminaci√≥n\n",
    "Se aplica **CLAHE** sobre el canal de luminosidad (espacio LAB) para aumentar el contraste en escenas nocturnas o con poca luz, mejorando la detecci√≥n sin amplificar excesivamente el ruido.\n",
    "\n",
    "- ### üö¶ An√°lisis del estado del sem√°foro\n",
    "Se determina si un sem√°foro est√° en **ROJO** o **VERDE** mediante segmentaci√≥n por color en el espacio **HSV**, contabilizando p√≠xeles relevantes. Es una soluci√≥n ligera, r√°pida y suficiente para un elemento cr√≠tico de seguridad.\n",
    "\n",
    "- ### üìè Estimaci√≥n de distancia\n",
    "La distancia a los objetos se estima a partir de la posici√≥n vertical de su base en la imagen, utilizando la *Flat Earth Assumption*. Aunque aproximada, esta t√©cnica permite clasificar riesgos cercanos sin necesidad de sensores adicionales.\n",
    "\n",
    "- ### üö∏ Detecci√≥n de pasos de cebra\n",
    "Se identifican patrones blancos rectangulares en la parte inferior de la imagen mediante umbralizaci√≥n y contornos, resaltando visualmente zonas seguras de cruce sin recurrir a modelos pesados.\n",
    "\n",
    "- ### üõ∞Ô∏è Radar visual de proximidad\n",
    "Se muestra un **minimapa tipo radar** que representa la posici√≥n relativa y distancia de los objetos detectados desde una vista cenital simplificada, facilitando una comprensi√≥n espacial r√°pida e intuitiva.\n",
    "\n",
    "- ### üìù OCR selectivo en im√°genes\n",
    "El reconocimiento de texto se aplica √∫nicamente sobre **im√°genes est√°ticas**, detectando previamente regiones candidatas (carteles) por criterios geom√©tricos y de luminosidad, lo que maximiza la precisi√≥n y reduce el coste computacional.\n",
    "\n",
    "- ### üîä Generaci√≥n de audios pregrabados\n",
    "Las alertas m√°s comunes se generan previamente mediante TTS y se almacenan como archivos de audio, evitando s√≠ntesis en tiempo real y garantizando **baja latencia** en situaciones cr√≠ticas.\n",
    "\n",
    "En conjunto, estas funciones act√∫an como una **capa de apoyo contextual** que mejora la robustez, claridad y eficiencia del sistema sin comprometer el rendimiento en tiempo real.\n",
    "\n",
    "### ‚ö†Ô∏è Limitaci√≥n: Se√±ales de Tr√°fico\n",
    "\n",
    "El sistema s√≠ detecta **se√±ales de STOP** y les hace Zoom, pero est√° limitado a esa clase espec√≠fica del dataset COCO. Si se desea detectar m√°s tipos de se√±ales de tr√°fico, necesitar√≠amos un modelo YOLO entrenado espec√≠ficamente para se√±ales de tr√°fico (como los de datasets de conducci√≥n aut√≥noma). Quiz√° esto resulte m√°s interesante desde un punto de vista de conductor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d50e57",
   "metadata": {},
   "source": [
    "## 3. Definici√≥n del Pipeline de Procesamiento\n",
    "Aqu√≠ se define la funci√≥n principal `procesar_video_safestep` que integra todo el flujo de trabajo: preprocesamiento, detecci√≥n, OCR, l√≥gica de seguridad y generaci√≥n de video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fba1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£ FUNCI√ìN DE PROCESAMIENTO (Definici√≥n)\n",
    "def procesar_video_safestep(ruta_video_entrada, ruta_audio, ruta_video_salida=None):\n",
    "    \"\"\"\n",
    "    Procesa un video individual aplicando todo el pipeline de SafeStep.\n",
    "    Si no se especifica ruta_video_salida, se genera autom√°ticamente.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(ruta_video_entrada):\n",
    "        print(f\"‚ùå Error: No se encuentra el video {ruta_video_entrada}\")\n",
    "        return None\n",
    "\n",
    "    # Extraer nombre base del video para usar en archivos temporales\n",
    "    nombre_base = os.path.splitext(os.path.basename(ruta_video_entrada))[0]\n",
    "\n",
    "    # --- TRANSCRIPCI√ìN DE LA ORDEN DE AUDIO A TEXTO ---\n",
    "    orden_usuario = transcribir_audio(ruta_audio)\n",
    "    orden = orden_usuario.lower()\n",
    "\n",
    "    # --- INTERPRETACI√ìN DE LA ORDEN (NLP B√ÅSICO) ---\n",
    "    modo_filtrado = analizar_intencion_spacy(orden_usuario)\n",
    "    print(f\"üß† Orden recibida: '{orden_usuario}' -> Modo: {modo_filtrado}\")\n",
    "\n",
    "    # Generar nombre de salida autom√°tico si no se da\n",
    "    if ruta_video_salida is None:\n",
    "        # Crear carpeta de salida si no existe\n",
    "        output_dir = \"videos_safestep\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        # Generar ruta de salida\n",
    "        nombre = os.path.splitext(os.path.basename(ruta_video_entrada))[0]\n",
    "        ruta_video_salida = os.path.join(output_dir, f\"{nombre}_safestep.mp4\")\n",
    "\n",
    "    print(f\"üé¨ Procesando video: {ruta_video_entrada} -> {ruta_video_salida}\")\n",
    "    \n",
    "    # --- CARGA DEL VIDEO Y PAR√ÅMETROS INICIALES ---\n",
    "    clip = VideoFileClip(ruta_video_entrada)\n",
    "    fps_original = clip.fps\n",
    "    w, h = clip.size\n",
    "    \n",
    "    # --- OPTIMIZACI√ìN DE FPS ---\n",
    "    # Si el video tiene muchos FPS (>50), procesamos uno de cada dos frames\n",
    "    # para mantener el rendimiento cercano al tiempo real (~30 FPS).\n",
    "    saltar_frames = 1\n",
    "    if fps_original > 50:\n",
    "        saltar_frames = 2\n",
    "        print(\"‚ö° Modo Optimizado: Saltando frames para mantener ~30 FPS.\")\n",
    "    \n",
    "    fps_salida = fps_original / saltar_frames\n",
    "\n",
    "    # --- GESTI√ìN DE MEMORIA EFICIENTE (STREAMING TO DISK) ---\n",
    "    # En lugar de guardar todos los frames procesados en una lista en RAM (que causar√≠a Memory Leak),\n",
    "    # escribimos cada frame procesado directamente al disco duro en un archivo temporal.\n",
    "    # Crear carpeta temporal si no existe para no ensuciar el directorio ra√≠z\n",
    "    temp_dir = \"temp\"\n",
    "    if not os.path.exists(temp_dir):\n",
    "        os.makedirs(temp_dir)\n",
    "        \n",
    "    temp_video_path = os.path.join(temp_dir, f\"temp_proc_{nombre_base}.mp4\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_writer = cv2.VideoWriter(temp_video_path, fourcc, fps_salida, (w, h))\n",
    "\n",
    "    # --- GESTI√ìN DE AUDIO INTELIGENTE (PRIORIDADES + COOLDOWNS) ---\n",
    "    audio_events = [] \n",
    "    track_history = {} \n",
    "    ultimos_textos_detectados = [] \n",
    "    \n",
    "    # Configuraci√≥n de prioridades (1 = M√°xima, 3 = M√≠nima)\n",
    "    PRIORIDADES = {\n",
    "        \"PELIGRO_MOVIMIENTO\": 1, \"SEMAFORO_ROJO\": 1,\n",
    "        \"PEATONES\": 2, \"SEMAFORO_VERDE\": 2,\n",
    "        \"SE√ëAL\": 3, \"COCHE_QUIETO\": 3\n",
    "    }\n",
    "    \n",
    "    # Tiempos de espera (segundos) antes de repetir el MISMO aviso\n",
    "    COOLDOWNS = {\n",
    "        \"PELIGRO_MOVIMIENTO\": 8,  # No repetir constantemente\n",
    "        \"SEMAFORO_ROJO\": 6,       # Importante recordar si sigue rojo\n",
    "        \"PEATONES\": 5,\n",
    "        \"SEMAFORO_VERDE\": 8,\n",
    "        \"SE√ëAL\": 10,\n",
    "        \"COCHE_QUIETO\": 15        # Poco relevante, cooldown largo\n",
    "    }\n",
    "    \n",
    "    # Estado del sistema de audio\n",
    "    last_trigger_time = {k: -999 for k in COOLDOWNS} # √öltima vez que son√≥ cada tipo\n",
    "    global_audio_finish_time = 0 # Momento en que el canal de audio queda libre\n",
    "    \n",
    "    \"\"\" \n",
    "    def trigger_audio_event(event_key, current_time):\n",
    "        nonlocal global_audio_finish_time\n",
    "        prioridad = PRIORIDADES.get(event_key, 3)\n",
    "        cooldown = COOLDOWNS.get(event_key, 10)\n",
    "        last_time = last_trigger_time.get(event_key, -999)\n",
    "        \n",
    "        # Verificar cooldown\n",
    "        if current_time - last_time < cooldown:\n",
    "            return\n",
    "        \n",
    "        # Verificar si el canal de audio est√° libre o si la nueva prioridad es mayor\n",
    "        if current_time >= global_audio_finish_time or prioridad < track_history.get(\"current_priority\", 4):\n",
    "            audio_path = f\"audios/aviso_{event_key}.mp3\"\n",
    "            audio_clip = AudioFileClip(audio_path)\n",
    "            audio_events.append((current_time, audio_clip))\n",
    "            global_audio_finish_time = current_time + audio_clip.duration\n",
    "            last_trigger_time[event_key] = current_time\n",
    "            track_history[\"current_priority\"] = prioridad \n",
    "\"\"\"\n",
    "\n",
    "    # --- BUCLE PRINCIPAL DE PROCESAMIENTO ---\n",
    "    for i, frame in enumerate(clip.iter_frames()):\n",
    "        if i % saltar_frames != 0: continue\n",
    "        \n",
    "        current_time = i / fps_original\n",
    "        \n",
    "        # 1. PREPROCESAMIENTO\n",
    "        # Mejorar contraste para visi√≥n nocturna\n",
    "        frame_mejorado = mejorar_contraste_noche(frame)\n",
    "        \n",
    "        # Dibujar paso de cebra SOLO si es relevante\n",
    "        if modo_filtrado in [\"TODO\", \"PASO_CEBRA\"]:\n",
    "            img_out = dibujar_paso_cebra_seguro(frame_mejorado)\n",
    "        else:\n",
    "            img_out = frame_mejorado.copy()\n",
    "            \n",
    "        h_img, w_img = img_out.shape[:2]\n",
    "        \n",
    "        # 2. TRACKING DE OBJETOS (YOLOv8)\n",
    "        # Ejecutamos YOLO primero para saber D√ìNDE est√°n los objetos.\n",
    "        # Usamos el tracker \"bytetrack\" para mantener la identidad de los objetos entre frames.\n",
    "        results = model_yolo.track(frame_mejorado, classes=CLASES_TRAFICO, persist=True, verbose=False, conf=0.15, tracker=\"bytetrack.yaml\")\n",
    "        \n",
    "        # 3. OCR SELECTIVO (Triggered OCR) - OPTIMIZACI√ìN CR√çTICA\n",
    "        # El OCR es muy lento. En lugar de leer toda la imagen, solo leemos\n",
    "        # si YOLO detecta una se√±al de tr√°fico (Clase 11).\n",
    "        # Adem√°s, solo ejecutamos OCR cada 15 frames para ahorrar recursos.\n",
    "        if modo_filtrado in [\"TODO\", \"SE√ëALES\"] and i % 30 == 0:\n",
    "            ultimos_textos_detectados = []\n",
    "\n",
    "            # Convertir frame a RGB si no lo est√°\n",
    "            frame_rgb = cv2.cvtColor(frame_mejorado, cv2.COLOR_BGR2RGB)\n",
    "            h_img, w_img = frame_rgb.shape[:2]\n",
    "\n",
    "            # --- Detectar zonas claras similares a carteles ---\n",
    "            hsv = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2HSV)\n",
    "            mask = cv2.inRange(hsv, (0, 0, 140), (180, 80, 255))\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 8))\n",
    "            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            for cnt in contours:\n",
    "                x, y, wc, hc = cv2.boundingRect(cnt)\n",
    "                area = wc * hc\n",
    "                aspect = wc / max(hc, 1)\n",
    "\n",
    "                # Filtrar zonas demasiado peque√±as o proporciones extra√±as\n",
    "                if area < 400 or aspect < 1.5 or aspect > 12 or y > h_img * 0.65:\n",
    "                    continue\n",
    "\n",
    "                margin = 8\n",
    "                x1, y1 = max(0, x - margin), max(0, y - margin)\n",
    "                x2, y2 = min(w_img, x + wc + margin), min(h_img, y + hc + margin)\n",
    "                roi = frame_rgb[y1:y2, x1:x2]\n",
    "                roi_big = cv2.resize(roi, None, fx=5, fy=5, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "                try:\n",
    "                    lecturas = reader.readtext(roi_big, paragraph=False, min_size=5,\n",
    "                                            text_threshold=0.35, low_text=0.25, width_ths=0.9)\n",
    "                    for bbox, text, prob in lecturas:\n",
    "                        if prob < 0.4:  # ignorar confianza baja\n",
    "                            continue\n",
    "                        # Reescalar bbox al tama√±o del frame original\n",
    "                        escala_x = (x2 - x1) / roi_big.shape[1]\n",
    "                        escala_y = (y2 - y1) / roi_big.shape[0]\n",
    "                        bbox_scaled = [[int(px * escala_x + x1), int(py * escala_y + y1)] for px, py in bbox]\n",
    "                        ultimos_textos_detectados.append((bbox_scaled, text, prob))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error OCR en ROI: {e}\")\n",
    "\n",
    "        # Dibujar textos detectados (Persistencia visual para que no parpadeen)\n",
    "        # SOLO si el modo incluye lectura de se√±ales\n",
    "        if modo_filtrado in [\"TODO\", \"SE√ëALES\"]:\n",
    "            for (bbox, text, prob) in ultimos_textos_detectados:\n",
    "                (tl, tr, br, bl) = bbox\n",
    "                x1, y1 = int(tl[0]), int(tl[1]); x2, y2 = int(br[0]), int(br[1])\n",
    "                cv2.rectangle(img_out, (x1, y1), (x2, y2), (255, 100, 0), 2)\n",
    "                cv2.putText(img_out, text.upper(), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.8, (255, 100, 0), 4)\n",
    "                \n",
    "                # Efecto Lupa: Mostrar el texto ampliado en una esquina\n",
    "                if (x2 - x1) < 100: \n",
    "                    try:\n",
    "                        roi = frame_mejorado[y1:y2, x1:x2]\n",
    "                        zoom_size = 200\n",
    "                        roi_zoom = cv2.resize(roi, (zoom_size, zoom_size))\n",
    "                        roi_zoom = cv2.copyMakeBorder(roi_zoom, 5, 5, 5, 5, cv2.BORDER_CONSTANT, value=(255, 100, 0))\n",
    "                        y_off, x_off = h_img - zoom_size - 20, w_img - zoom_size - 20\n",
    "                        img_out[y_off:y_off+zoom_size+10, x_off:x_off+zoom_size+10] = roi_zoom\n",
    "                        cv2.putText(img_out, \"TEXTO\", (x_off, y_off - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 100, 0), 2)\n",
    "                    except: pass\n",
    "\n",
    "        # 4. L√ìGICA DE SEGURIDAD Y VISUALIZACI√ìN\n",
    "        frame_alerts = [] # Lista de alertas detectadas en ESTE frame\n",
    "        mensaje_visual = \"\"\n",
    "        color_mensaje = (255, 255, 255)\n",
    "        peligro_inminente = False\n",
    "        objetos_radar = [] \n",
    "\n",
    "        if results and results[0]:\n",
    "            boxes = results[0].boxes\n",
    "            if boxes.id is not None:\n",
    "                track_ids = boxes.id.int().cpu().tolist()\n",
    "                cls_ids = boxes.cls.int().cpu().tolist()\n",
    "                coords = boxes.xyxy.cpu().numpy()\n",
    "                \n",
    "                for box_id, cls, xyxy in zip(track_ids, cls_ids, coords):\n",
    "                    x1, y1, x2, y2 = map(int, xyxy)\n",
    "                    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "                    \n",
    "                    # Estimar distancia\n",
    "                    distancia_m = estimar_distancia(y2, h_img)\n",
    "                    texto_dist = f\"{distancia_m}m\"\n",
    "                    \n",
    "                    # --- L√ìGICA DE MOVIMIENTO (Siempre calcular para mantener estado) ---\n",
    "                    es_movimiento = False\n",
    "                    if cls in [2, 3, 5, 7]: \n",
    "                        if box_id in track_history:\n",
    "                            prev_x, prev_y = track_history[box_id]\n",
    "                            desplazamiento = np.sqrt((center_x - prev_x)**2 + (center_y - prev_y)**2)\n",
    "                            if desplazamiento > (5.0 * saltar_frames): es_movimiento = True\n",
    "                        track_history[box_id] = (center_x, center_y)\n",
    "\n",
    "                    # --- FILTRADO VISUAL: ¬øDebemos mostrar este objeto? ---\n",
    "                    mostrar_visual = False\n",
    "                    if modo_filtrado == \"TODO\": mostrar_visual = True\n",
    "                    elif modo_filtrado == \"VEHICULOS\" and cls in [2, 3, 5, 7]: mostrar_visual = True\n",
    "                    elif modo_filtrado == \"PASO_CEBRA\" and cls == 0: mostrar_visual = True # Peatones relevantes para cruce\n",
    "                    elif modo_filtrado == \"SEMAFORO\" and cls == 9: mostrar_visual = True\n",
    "                    elif modo_filtrado == \"SE√ëALES\" and cls == 11: mostrar_visual = True\n",
    "                    elif modo_filtrado == \"SOLO_PELIGRO\" and es_movimiento: mostrar_visual = True\n",
    "                    \n",
    "                    if not mostrar_visual: \n",
    "                        continue\n",
    "\n",
    "                    # Si pasa el filtro, lo a√±adimos al radar\n",
    "                    objetos_radar.append((int(cls), center_x, distancia_m))\n",
    "                    \n",
    "                    # --- L√ìGICA PARA VEH√çCULOS ---\n",
    "                    if cls in [2, 3, 5, 7]: \n",
    "                        color_box = (200, 0, 0) if es_movimiento else (0, 0, 255)\n",
    "                        cv2.rectangle(img_out, (x1, y1), (x2, y2), color_box, 8)\n",
    "                        cv2.putText(img_out, texto_dist, (x1, y1 - 35), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 0), 4)\n",
    "                        \n",
    "                        #-- L√ìGICA DE ALERTAS PARA VEH√çCULOS ---\n",
    "                        if es_movimiento:\n",
    "                            frame_alerts.append(\"PELIGRO_MOVIMIENTO\")\n",
    "                            mensaje_visual = f\"PELIGRO: VEHICULO A {distancia_m}m\"\n",
    "                            color_mensaje = (255, 0, 0); peligro_inminente = True\n",
    "                        else: frame_alerts.append(\"COCHE_QUIETO\")\n",
    "                            \n",
    "                    # --- L√ìGICA PARA PEATONES ---\n",
    "                    elif cls == 0: \n",
    "                        if (y2 - y1) > h_img * 0.3: # Solo si est√° cerca/grande\n",
    "                            frame_alerts.append(\"PEATONES\")\n",
    "                            if \"PELIGRO_MOVIMIENTO\" not in frame_alerts:\n",
    "                                mensaje_visual = \"CUIDADO: PEATONES\"\n",
    "                                color_mensaje = (255, 255, 0)\n",
    "                        # Amarillo (255, 255, 0) para alta visibilidad y precauci√≥n\n",
    "                        cv2.rectangle(img_out, (x1, y1), (x2, y2), (255, 255, 0), 8)\n",
    "\n",
    "                    # --- L√ìGICA PARA SEM√ÅFOROS Y SE√ëALES ---\n",
    "                    elif cls in [9, 11]:\n",
    "                        roi = frame_mejorado[y1:y2, x1:x2]\n",
    "                        color_borde = (255, 165, 0)\n",
    "                        if cls == 9: # Sem√°foro\n",
    "                            estado_sem = analizar_estado_semaforo(roi)\n",
    "                            if estado_sem == \"ROJO\":\n",
    "                                frame_alerts.append(\"SEMAFORO_ROJO\")\n",
    "                                mensaje_visual = \"SEMAFORO ROJO: ESPERE\"\n",
    "                                color_mensaje = (255, 0, 0); color_borde = (255, 0, 0)\n",
    "                            elif estado_sem == \"VERDE\":\n",
    "                                frame_alerts.append(\"SEMAFORO_VERDE\")\n",
    "                                if \"PELIGRO_MOVIMIENTO\" not in frame_alerts and \"SEMAFORO_ROJO\" not in frame_alerts:\n",
    "                                    mensaje_visual = \"VERDE: PUEDE CRUZAR\"\n",
    "                                    color_mensaje = (0, 255, 0); color_borde = (0, 255, 0)\n",
    "                        \n",
    "                        # Dibujar Lupa para ver mejor la se√±al/sem√°foro\n",
    "                        try: \n",
    "                            zoom_size = 200\n",
    "                            roi_zoom = cv2.resize(roi, (zoom_size, zoom_size))\n",
    "                            roi_zoom = cv2.copyMakeBorder(roi_zoom, 5, 5, 5, 5, cv2.BORDER_CONSTANT, value=color_borde)\n",
    "                            y_off, x_off = 20, w_img - zoom_size - 20\n",
    "                            if y_off + zoom_size + 10 < h_img and x_off + zoom_size + 10 < w_img:\n",
    "                                img_out[y_off:y_off+zoom_size+10, x_off:x_off+zoom_size+10] = roi_zoom\n",
    "                                cv2.putText(img_out, \"ZOOM\", (x_off, y_off - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color_borde, 2)\n",
    "                        except: pass\n",
    "                        cv2.rectangle(img_out, (x1, y1), (x2, y2), color_borde, 8)\n",
    "\n",
    "        # 5. RADAR Y FEEDBACK FINAL\n",
    "        img_out = dibujar_radar(img_out, objetos_radar)\n",
    "        \n",
    "        # Alerta visual de pantalla completa si hay peligro inminente\n",
    "        # SOLO si estamos en un modo que muestre peligros o TODO\n",
    "        if peligro_inminente and modo_filtrado in [\"TODO\", \"VEHICULOS\", \"SOLO_PELIGRO\"]:\n",
    "             cv2.rectangle(img_out, (0, 0), (w_img, h_img), (0, 0, 255), 10)\n",
    "\n",
    "        # --- PROCESADOR DE AUDIO CENTRALIZADO ---\n",
    "        aviso_final = None\n",
    "        \n",
    "        # 0. Filtrado por Intenci√≥n del Usuario (NLP)\n",
    "        alertas_filtradas = []\n",
    "        for a in frame_alerts:\n",
    "            # Siempre permitimos PELIGRO_MOVIMIENTO por seguridad cr√≠tica\n",
    "            if a == \"PELIGRO_MOVIMIENTO\": \n",
    "                alertas_filtradas.append(a)\n",
    "                continue\n",
    "                \n",
    "            if modo_filtrado == \"TODO\":\n",
    "                alertas_filtradas.append(a)\n",
    "            elif modo_filtrado == \"SEMAFORO\" and \"SEMAFORO\" in a:\n",
    "                alertas_filtradas.append(a)\n",
    "            elif modo_filtrado == \"VEHICULOS\" and (\"COCHE\" in a or \"MOVIMIENTO\" in a):\n",
    "                alertas_filtradas.append(a)\n",
    "            elif modo_filtrado == \"PASO_CEBRA\" and \"PEATONES\" in a: # Peatones suelen implicar cruce\n",
    "                alertas_filtradas.append(a)\n",
    "            elif modo_filtrado == \"SE√ëALES\" and \"SE√ëAL\" in a:\n",
    "                alertas_filtradas.append(a)\n",
    "            elif modo_filtrado == \"SOLO_PELIGRO\":\n",
    "                pass # Ya se a√±adi√≥ PELIGRO_MOVIMIENTO arriba\n",
    "        \n",
    "        # 1. Filtrar alertas por Cooldown (¬øHace cu√°nto son√≥ esta misma alerta?)\n",
    "        alertas_disponibles = [a for a in alertas_filtradas if (current_time - last_trigger_time.get(a, -999)) > COOLDOWNS.get(a, 5)]\n",
    "        \n",
    "        # 2. Ordenar por Prioridad (Menor n√∫mero = Mayor importancia)\n",
    "        if alertas_disponibles:\n",
    "            alertas_disponibles.sort(key=lambda x: PRIORIDADES.get(x, 99))\n",
    "            mejor_alerta = alertas_disponibles[0] # La m√°s importante\n",
    "            \n",
    "            # 3. Chequear disponibilidad del canal de audio (¬øHa terminado el anterior?)\n",
    "            # Permitimos solapamiento leve (0.5s) para alertas de Prioridad 1\n",
    "            margen = 0.5 if PRIORIDADES.get(mejor_alerta, 99) == 1 else 0\n",
    "            \n",
    "            if current_time >= (global_audio_finish_time - margen):\n",
    "                aviso_final = mejor_alerta\n",
    "                \n",
    "                # Registrar evento\n",
    "                audio_file = f\"audios/aviso_{aviso_final}.mp3\"\n",
    "                if os.path.exists(audio_file):\n",
    "                    audio_events.append((current_time, audio_file))\n",
    "                    \n",
    "                    # Actualizar tiempos\n",
    "                    last_trigger_time[aviso_final] = current_time\n",
    "                    # Estimamos duraci√≥n del audio en 2.5s (promedio)\n",
    "                    global_audio_finish_time = current_time + 2.5 \n",
    "                    print(f\"üîä {aviso_final} en {current_time:.2f}s (Prioridad: {PRIORIDADES.get(aviso_final)})\")\n",
    "\n",
    "        # Dibujar mensaje de texto inferior (Aumentado para baja visi√≥n)\n",
    "        if mensaje_visual:\n",
    "            # Escala reducida para no ocupar toda la pantalla\n",
    "            font_scale = 1.2\n",
    "            thickness = 3\n",
    "            (text_w, text_h), baseline = cv2.getTextSize(mensaje_visual, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)\n",
    "            \n",
    "            # Fondo negro de alto contraste m√°s grande\n",
    "            cv2.rectangle(img_out, (10, h_img - text_h - 40), (10 + text_w + 40, h_img - 10), (0, 0, 0), -1)\n",
    "            # Borde blanco grueso para resaltar sobre fondo oscuro\n",
    "            cv2.rectangle(img_out, (10, h_img - text_h - 40), (10 + text_w + 40, h_img - 10), (255, 255, 255), 4)\n",
    "            \n",
    "            cv2.putText(img_out, mensaje_visual, (30, h_img - 25), cv2.FONT_HERSHEY_SIMPLEX, font_scale, color_mensaje, thickness)\n",
    "\n",
    "        # ESCRIBIR FRAME A DISCO (Evitar Memory Leak)\n",
    "        # Convertir RGB (MoviePy) a BGR (OpenCV) antes de escribir\n",
    "        out_writer.write(cv2.cvtColor(img_out, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        if i % 10 == 0: print(f\"Procesado frame {i}...\", end=\"\\r\")\n",
    "\n",
    "    out_writer.release()\n",
    "    print(\"\\nüíæ Generando video final con audio...\")\n",
    "    \n",
    "    # --- POST-PROCESADO DE AUDIO ---\n",
    "    # Cargar el video mudo procesado y a√±adirle los eventos de audio\n",
    "    clip_video = VideoFileClip(temp_video_path)\n",
    "    \n",
    "    if audio_events:\n",
    "        audioclips = []\n",
    "        for t, audio_file in audio_events:\n",
    "            try:\n",
    "                aclip = AudioFileClip(audio_file)\n",
    "                aclip = aclip.with_start(t) if IS_MOVIEPY_2 else aclip.set_start(t)\n",
    "                audioclips.append(aclip)\n",
    "            except: pass\n",
    "        if audioclips:\n",
    "            final_audio = CompositeAudioClip(audioclips)\n",
    "            final_audio = final_audio.with_duration(clip_video.duration) if IS_MOVIEPY_2 else final_audio.set_duration(clip_video.duration)\n",
    "            clip_video = clip_video.with_audio(final_audio) if IS_MOVIEPY_2 else clip_video.set_audio(final_audio)\n",
    "\n",
    "    clip_video.write_videofile(ruta_video_salida, codec=\"libx264\", audio_codec=\"aac\")\n",
    "    \n",
    "   # --- CIERRE EXPL√çCITO DE RECURSOS (CR√çTICO EN WINDOWS) ---\n",
    "    try:\n",
    "        clip_video.close()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        clip.close()  # cerrar el video de entrada\n",
    "    except:\n",
    "        pass\n",
    "    # Forzar liberaci√≥n de memoria y handles\n",
    "    gc.collect()\n",
    "    time.sleep(0.5)\n",
    "    # --- LIMPIEZA DE ARCHIVOS TEMPORALES ---\n",
    "    if os.path.exists(temp_video_path):\n",
    "        try:\n",
    "            os.remove(temp_video_path)\n",
    "        except PermissionError:\n",
    "            print(\"‚ö†Ô∏è No se pudo borrar el archivo temporal (Windows lo mantiene abierto)\")\n",
    "    # Intentar borrar carpeta temp si est√° vac√≠a\n",
    "    try:\n",
    "        os.rmdir(temp_dir)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    print(f\"‚úÖ Video guardado: {ruta_video_salida}\")\n",
    "    return ruta_video_salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d611c88c",
   "metadata": {},
   "source": [
    "## üé¨ Funci√≥n de Procesamiento Principal ‚Äî `procesar_video_safestep`\n",
    "\n",
    "Esta funci√≥n implementa el **pipeline completo de SafeStep** para procesar un v√≠deo de forma eficiente, segura y adaptada a la intenci√≥n del usuario.\n",
    "\n",
    "### üéôÔ∏è Entrada multimodal\n",
    "- Transcribe comandos de voz con **Whisper**.\n",
    "- Interpreta la intenci√≥n mediante **NLP (SpaCy)** para activar modos selectivos (veh√≠culos, se√±ales, sem√°foros, etc.).\n",
    "\n",
    "### ‚ö° Procesamiento optimizado\n",
    "- Ajuste din√°mico de FPS para mantener rendimiento cercano al tiempo real.\n",
    "- Escritura directa a disco (*streaming to disk*) para evitar consumo excesivo de memoria.\n",
    "\n",
    "### üëÅÔ∏è Visi√≥n artificial en tiempo real\n",
    "- **YOLOv8 + ByteTrack** para detecci√≥n y seguimiento de objetos.\n",
    "- Mejora de contraste nocturno y detecci√≥n opcional de pasos de cebra.\n",
    "- **OCR selectivo** ejecutado cada N frames y solo sobre los v√≠deos cuya intenci√≥n del usuario sea carteles/se√±ales (SE√ëALES) y cualquier alerta (TODO).\n",
    "\n",
    "### üö® L√≥gica de seguridad\n",
    "- Estimaci√≥n de distancia y detecci√≥n de movimiento.\n",
    "- Priorizaci√≥n de alertas con sistema de **prioridades y cooldowns**.\n",
    "- Mensajes visuales de alto contraste y alertas de peligro inminente.\n",
    "\n",
    "### üõ∞Ô∏è Feedback visual y auditivo\n",
    "- Radar de proximidad para contexto espacial.\n",
    "- Reproducci√≥n de audios pregrabados sincronizados con el v√≠deo final.\n",
    "\n",
    "### üßπ Postprocesado robusto\n",
    "- Composici√≥n final de v√≠deo + audio.\n",
    "- Liberaci√≥n expl√≠cita de recursos y limpieza de archivos temporales.\n",
    "\n",
    "‚û°Ô∏è Resultado: un v√≠deo final anotado, accesible y optimizado, listo para asistir al usuario en entornos urbanos reales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7ad0b8",
   "metadata": {},
   "source": [
    "## 4. Ejecuci√≥n y Pruebas\n",
    "Bloque para ejecutar el pipeline sobre un video de prueba y verificar los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3701afad",
   "metadata": {},
   "source": [
    "### 4.1. Veh√≠culos en Movimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a3e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ EJECUCI√ìN MANUAL - VIDEO 1 (Tr√°fico)\n",
    "# Escenario: Usuario esperando en una parada o borde de acera.\n",
    "ruta_audio1 = \"audios_ordenes/orden1.mp3\"\n",
    "\n",
    "from IPython.display import Video, display\n",
    "video_a_procesar = \"videos/video1.mp4\"\n",
    "\n",
    "if os.path.exists(video_a_procesar):\n",
    "    ruta_salida = procesar_video_safestep(video_a_procesar, ruta_audio1)\n",
    "    if ruta_salida and os.path.exists(ruta_salida):\n",
    "        display(Video(ruta_salida, embed=True, html_attributes=\"controls width=640\"))\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se encuentra el archivo: {video_a_procesar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618da61",
   "metadata": {},
   "source": [
    "### 4.2. Pasos de cebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1925ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ EJECUCI√ìN MANUAL - VIDEO 2 (Navegaci√≥n)\n",
    "# Escenario: Usuario buscando cruzar la calle.\n",
    "ruta_audio2 = \"audios_ordenes/orden2.mp3\"\n",
    "\n",
    "from IPython.display import Video, display\n",
    "video_a_procesar = \"videos/video2.mp4\"\n",
    "\n",
    "if os.path.exists(video_a_procesar):\n",
    "    ruta_salida = procesar_video_safestep(video_a_procesar, ruta_audio2)\n",
    "    if ruta_salida and os.path.exists(ruta_salida):\n",
    "        display(Video(ruta_salida, embed=True, html_attributes=\"controls width=640\"))\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se encuentra el archivo: {video_a_procesar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9c92c",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Desaf√≠o: Falsos Positivos en la Detecci√≥n de Pasos de Cebra\n",
    "\n",
    "- **Problema:** Aunque la funci√≥n detecta correctamente muchos pasos de cebra reales, en ocasiones puede identificar como paso de cebra otros elementos del entorno, como franjas en el pavimento, marcas viales, sombras o incluso zonas de acera con patrones similares.\n",
    "\n",
    "- **Causa t√©cnica:** La funci√≥n `analizar_paso_cebra` se basa en la detecci√≥n de l√≠neas blancas paralelas y su disposici√≥n geom√©trica. Sin embargo, no incorpora un an√°lisis sem√°ntico profundo ni contexto urbano, por lo que cualquier patr√≥n visual que cumpla con los criterios geom√©tricos y de color puede ser interpretado err√≥neamente como paso de cebra.\n",
    "\n",
    "- **Ejemplo:** Unas losas blancas alineadas en una acera, o marcas de aparcamiento, pueden ser confundidas con pasos de cebra si su forma y color se parecen lo suficiente.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Justificaci√≥n y Posibles Mejoras\n",
    "\n",
    "- **Justificaci√≥n:** El m√©todo empleado es eficiente y r√°pido, pero limitado por su simplicidad. Es adecuado para prototipos y entornos controlados, pero puede fallar en escenarios urbanos complejos.\n",
    "\n",
    "- **Mejoras posibles:** Para reducir falsos positivos, se podr√≠an emplear modelos de aprendizaje profundo entrenados espec√≠ficamente para pasos de cebra, o combinar la detecci√≥n geom√©trica con informaci√≥n contextual (por ejemplo, la presencia de sem√°foros, cruces o se√±ales verticales)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84149912",
   "metadata": {},
   "source": [
    "### 4.3. Sem√°foros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ EJECUCI√ìN MANUAL - VIDEO 3 (Sem√°foros)\n",
    "# Escenario: Usuario en un cruce regulado, quiere saber cu√°ndo cruzar.\n",
    "ruta_audio3 = \"audios_ordenes/orden3.mp3\"\n",
    "\n",
    "from IPython.display import Video, display\n",
    "video_a_procesar = \"videos/video3.mp4\"\n",
    "\n",
    "if os.path.exists(video_a_procesar):\n",
    "    ruta_salida = procesar_video_safestep(video_a_procesar, ruta_audio3)\n",
    "    if ruta_salida and os.path.exists(ruta_salida):\n",
    "        display(Video(ruta_salida, embed=True, html_attributes=\"controls width=640\"))\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se encuentra el archivo: {video_a_procesar}\")\n",
    "\n",
    "# 3Ô∏è‚É£ EJECUCI√ìN MANUAL - VIDEO 4 (Sem√°foros)\n",
    "video_a_procesar = \"videos/video4.mp4\"\n",
    "\n",
    "if os.path.exists(video_a_procesar):\n",
    "    ruta_salida = procesar_video_safestep(video_a_procesar, ruta_audio3)\n",
    "    if ruta_salida and os.path.exists(ruta_salida):\n",
    "        display(Video(ruta_salida, embed=True, html_attributes=\"controls width=640\"))\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se encuentra el archivo: {video_a_procesar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9433df",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è An√°lisis de Caso de Borde: Falso Positivo en Video 4\n",
    "\n",
    "Durante la ejecuci√≥n del `video4.mp4`, se puede observar una **detecci√≥n err√≥nea de sem√°foro**.\n",
    "\n",
    "**Justificaci√≥n T√©cnica:**\n",
    "El sistema identifica correctamente una se√±al de **STOP**, pero el algoritmo de an√°lisis de estado (`analizar_estado_semaforo`) confunde la gran cantidad de p√≠xeles rojos de la se√±al de STOP con la luz roja de un sem√°foro. Esto ocurre porque la l√≥gica de detecci√≥n de color busca picos de rojo en la regi√≥n de inter√©s, y al ser la se√±al de STOP predominantemente roja, genera un falso positivo de \"Sem√°foro en Rojo\".\n",
    "\n",
    "El `video3.mp4` se incluye como contraprueba para demostrar el funcionamiento correcto del algoritmo en un escenario donde no existe esta ambig√ºedad visual, validando la l√≥gica de detecci√≥n de sem√°foros en condiciones normales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e399172",
   "metadata": {},
   "source": [
    "### 4.4. Carteles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9612fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ EJECUCI√ìN MANUAL - VIDEO 5 (Carteles)\n",
    "# Escenario: Usuario en entorno desconocido, quiere indicaci√≥n de carteles.\n",
    "ruta_audio4 = \"audios_ordenes/orden4.mp3\"\n",
    "\n",
    "from IPython.display import Video, display\n",
    "video_a_procesar = \"videos/video5.mp4\"\n",
    "\n",
    "if os.path.exists(video_a_procesar):\n",
    "    ruta_salida = procesar_video_safestep(video_a_procesar, ruta_audio4)\n",
    "    if ruta_salida and os.path.exists(ruta_salida):\n",
    "        display(Video(ruta_salida, embed=True, html_attributes=\"controls width=640\"))\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se encuentra el archivo: {video_a_procesar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e4c29",
   "metadata": {},
   "source": [
    "### ‚ùì Por qu√© el OCR de carteles en v√≠deo puede fallar\n",
    "\n",
    "En la ejecuci√≥n sobre el **video 5 (carteles)**, el sistema no logra detectar en su totalidad el texto del cartel. Esto puede deberse a varias causas t√©cnicas habituales en visi√≥n artificial:\n",
    "\n",
    "- **Baja resoluci√≥n del video:** Los carteles aparecen demasiado peque√±os para que el OCR pueda distinguir los caracteres.\n",
    "- **Desenfoque o movimiento:** El v√≠deo puede estar borroso, dificultando la segmentaci√≥n y lectura.\n",
    "- **Iluminaci√≥n deficiente o reflejos:** Sombras, contraluces o brillos pueden ocultar parte del texto.\n",
    "- **Compresi√≥n y artefactos:** Los v√≠deos suelen tener artefactos de compresi√≥n que degradan la calidad de los bordes y el contraste.\n",
    "- **√Ångulo o perspectiva:** Si el cartel est√° inclinado o deformado por la perspectiva, el OCR puede fallar.\n",
    "- **Ruido visual:** Elementos del fondo, otros objetos o textos cercanos pueden confundir al sistema.\n",
    "\n",
    "Adem√°s, al hacer uso de OCR cada 30 frames (aprox. cada 1 segundos) en toda la imagen, el procesamiento es muy lento. Se han probado diferentes t√©cnicas para su correcci√≥n, como analizar solo el 40% de la imagen superior, donde suelen estar los carteles, pero esta decisi√≥n representaba una limitaci√≥n enorme en cuanto a la detecci√≥n de textos √∫tiles para el ciclo peatonal. Por ende, se ha de considerar su optimizaci√≥n para versiones m√°s avanzadas del proyecto actual.\n",
    "\n",
    "#### üñºÔ∏è OCR en im√°genes fijas: una alternativa robusta\n",
    "\n",
    "Para demostrar la utilidad y precisi√≥n del OCR en el contexto de SafeStep Peatonal, se ha a√±adido una herramienta espec√≠fica para **detectar y leer carteles en im√°genes fijas**. Esta t√©cnica permite:\n",
    "\n",
    "- Probar el OCR sobre capturas de pantalla o fotos de carteles tomadas con el m√≥vil.\n",
    "- Ajustar el preprocesamiento y la ampliaci√≥n para maximizar la legibilidad.\n",
    "- Visualizar el resultado con un dise√±o accesible para personas con baja visi√≥n.\n",
    "\n",
    "**Conclusi√≥n:**  \n",
    "Aunque el OCR en v√≠deo es un reto por las limitaciones t√©cnicas mencionadas, el reconocimiento de texto en im√°genes fijas funciona de forma fiable y puede integrarse en flujos de ayuda peatonal, por ejemplo, permitiendo al usuario tomar una foto de un cartel y recibir la lectura accesible del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba OCR en una imagen fija\n",
    "rutas = [\n",
    "    \"images/cartel1.jpg\",\n",
    "    \"images/cartel2.jpg\",\n",
    "    \"images/cartel3.png\",\n",
    "    \"images/cartel4.png\"\n",
    "]\n",
    "\n",
    "for ruta in rutas:\n",
    "    if os.path.exists(ruta):\n",
    "        print(f\"\\nüîç Probando OCR en imagen: {ruta}\")\n",
    "        img_resultado = ocr_carteles_images(ruta, reader, mostrar=True)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No se encuentra la imagen: {ruta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e084dfe",
   "metadata": {},
   "source": [
    "### 4.5. Alertas Cr√≠ticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9bdcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ EJECUCI√ìN MANUAL - VIDEO 6 (Modo Silencioso)\n",
    "# Escenario: Usuario en entorno ruidoso o conocido, solo quiere alertas cr√≠ticas.\n",
    "ruta_audio5 = \"audios_ordenes/orden5.mp3\"\n",
    "\n",
    "from IPython.display import Video, display\n",
    "video_a_procesar = \"videos/video6.mp4\"\n",
    "\n",
    "if os.path.exists(video_a_procesar):\n",
    "    ruta_salida = procesar_video_safestep(video_a_procesar, ruta_audio5)\n",
    "    if ruta_salida and os.path.exists(ruta_salida):\n",
    "        display(Video(ruta_salida, embed=True, html_attributes=\"controls width=640\"))\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se encuentra el archivo: {video_a_procesar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c873a6f0",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Desaf√≠o: Distinguir Movimiento Real de Objetos vs. Movimiento de C√°mara\n",
    "\n",
    "- **Problema:** Si la c√°mara no es est√°tica (por ejemplo, si el usuario camina o mueve el m√≥vil), los objetos pueden parecer que se mueven aunque est√©n quietos.\n",
    "- **Soluci√≥n profesional:** La mejor pr√°ctica ser√≠a compensar el movimiento global de la escena usando t√©cnicas como Optical Flow o Keypoints para estimar el desplazamiento de fondo y restarlo al movimiento de cada objeto. As√≠, solo se detecta movimiento real relativo al entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea9f978",
   "metadata": {},
   "source": [
    "### 4.6. General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e102e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ EJECUCI√ìN MANUAL - VIDEO 7 (General)\n",
    "# Escenario: Usuario caminando por calle normal. Quiere saber todo.\n",
    "ruta_audio6 = \"audios_ordenes/orden6.mp3\"\n",
    "\n",
    "from IPython.display import Video, display\n",
    "video_a_procesar = \"videos/video7.mp4\"\n",
    "\n",
    "if os.path.exists(video_a_procesar):\n",
    "    ruta_salida = procesar_video_safestep(video_a_procesar, ruta_audio6)\n",
    "    if ruta_salida and os.path.exists(ruta_salida):\n",
    "        display(Video(ruta_salida, embed=True, html_attributes=\"controls width=640\"))\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se encuentra el archivo: {video_a_procesar}\")\n",
    "\n",
    "# 3Ô∏è‚É£ EJECUCI√ìN MANUAL - VIDEO 8 (General)\n",
    "video_a_procesar = \"videos/video8.mp4\"\n",
    "\n",
    "if os.path.exists(video_a_procesar):\n",
    "    ruta_salida = procesar_video_safestep(video_a_procesar, ruta_audio6)\n",
    "    if ruta_salida and os.path.exists(ruta_salida):\n",
    "        display(Video(ruta_salida, embed=True, html_attributes=\"controls width=640\"))\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No se encuentra el archivo: {video_a_procesar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc238ccd",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Desaf√≠o: Sobrecarga de Informaci√≥n y Eficiencia\n",
    "\n",
    "- **Problema:** En entornos urbanos muy estimulados (muchos coches, peatones, se√±ales, sem√°foros, etc.), tratar de identificar y avisar de todo puede generar una sobrecarga de informaci√≥n y de avisos de audio.\n",
    "- **Consecuencia:** El usuario puede recibir demasiados avisos, lo que puede resultar confuso, molesto o incluso ineficiente para la toma de decisiones seguras.\n",
    "- **Reflexi√≥n:** Este es uno de los grandes retos de los sistemas de asistencia multimodal: encontrar el equilibrio entre informar y no saturar al usuario. En la pr√°ctica, ser√≠a necesario priorizar, filtrar y adaptar los avisos seg√∫n el contexto y las necesidades del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5d269",
   "metadata": {},
   "source": [
    "## 5. Justificaciones T√©cnicas y Decisiones de Dise√±o\n",
    "\n",
    "Este documento recoge las decisiones arquitect√≥nicas clave implementadas en **SafeStep** para equilibrar precisi√≥n, rendimiento y viabilidad t√©cnica.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚ö° Optimizaci√≥n de FPS (Frame Skipping)\n",
    "> **Problema:** El procesamiento de video en alta resoluci√≥n (1080p/60fps) satura la capacidad de procesamiento en tiempo real.  \n",
    "> **Soluci√≥n:** Implementaci√≥n de un algoritmo de salto de frames din√°mico.\n",
    "*   **Mecanismo:** Si el video de entrada supera los 50 FPS, el sistema procesa solo 1 de cada 2 frames.\n",
    "*   **Impacto:** Mantiene una tasa de salida estable de ~30 FPS, suficiente para la percepci√≥n humana, reduciendo la carga computacional a la mitad sin perder informaci√≥n cr√≠tica.\n",
    "\n",
    "### 2. üéØ OCR H√≠brido\n",
    "> **Problema:** El reconocimiento √≥ptico de caracteres (OCR) es computacionalmente costoso y propenso a leer \"ruido\" (texto en camisetas, carteles irrelevantes).  \n",
    "> **Soluci√≥n:** Solo se ejecuta cuando la intenci√≥n del usuario es SE√ëALES, enfoc√°ndose en carteles relevantes:\n",
    "*   **Modo SE√ëALES o TODO:** Escanea regiones del frame cada 30 frames para detectar texto de se√±alizaci√≥n urbana. No depende de YOLO.\n",
    "*   **Otros Modos:** El OCR no se ejecuta, evitando gasto computacional innecesario.\n",
    "*   **Impacto:** Minimiza carga de procesamiento y reduce falsos positivos, garantizando lectura de carteles solo cuando el usuario lo requiere..\n",
    "\n",
    "### 3. üíæ Gesti√≥n de Memoria (Streaming to Disk)\n",
    "> **Problema:** Almacenar frames procesados en listas de Python (`frames.append()`) provoca desbordamiento de memoria (RAM) en videos largos (>1 min), causando cierres inesperados.  \n",
    "> **Soluci√≥n:** Escritura en flujo continuo al disco.\n",
    "*   **Mecanismo:** Uso de `cv2.VideoWriter` para escribir cada frame procesado inmediatamente al almacenamiento secundario y liberar la memoria RAM al instante.\n",
    "*   **Impacto:** Consumo de RAM **constante y bajo** (O(1)), permitiendo procesar videos de duraci√≥n indefinida sin colapsar el sistema.\n",
    "\n",
    "### 4. üìè Estimaci√≥n de Distancia \"Flat Earth\"\n",
    "> **Problema:** Calcular la distancia real (profundidad) con una sola c√°mara (monocular) es un problema mal planteado sin referencias externas.  \n",
    "> **Soluci√≥n:** Aproximaci√≥n geom√©trica basada en la suposici√≥n de \"Tierra Plana\".\n",
    "*   **Mecanismo:** Se asume que la c√°mara est√° a una altura fija y el suelo es plano. La distancia se estima inversamente proporcional a la posici√≥n `y` de la base del objeto en la imagen (`d ~ 1 / (y - horizonte)`).\n",
    "*   **Impacto:** C√°lculo instant√°neo con error aceptable para alertas de seguridad (cerca/lejos), sin necesidad de sensores LiDAR o c√°maras est√©reo costosas.\n",
    "\n",
    "### 5. üß© Arquitectura Modular\n",
    "> **Problema:** La carga de modelos pesados (Whisper, YOLO, Spacy) tarda varios segundos.  \n",
    "> **Soluci√≥n:** Separaci√≥n del ciclo de vida.\n",
    "*   **Mecanismo:** La inicializaci√≥n de modelos se realiza en una celda independiente (\"Configuraci√≥n\") que se ejecuta una sola vez. La funci√≥n de procesamiento reutiliza estos objetos en memoria.\n",
    "*   **Impacto:** Permite iterar y procesar m√∫ltiples videos r√°pidamente sin tiempos de espera de carga repetitivos.\n",
    "\n",
    "### 6. üó£Ô∏è Integraci√≥n de Comandos de Voz y Procesamiento de Lenguaje Natural\n",
    "> **Problema:** La interacci√≥n por texto limita la accesibilidad y naturalidad del sistema.\n",
    "> **Soluci√≥n:** Uso de Whisper para transcribir √≥rdenes de voz y spaCy para interpretar la intenci√≥n del usuario.\n",
    "*   **Mecanismo:** El usuario puede grabar una orden hablada, que se transcribe a texto con Whisper y se analiza con spaCy para determinar el modo de funcionamiento.\n",
    "*   **Impacto:** Permite interacci√≥n multimodal, mayor accesibilidad y flexibilidad en la configuraci√≥n del sistema.\n",
    "\n",
    "### 7. üéß Gesti√≥n de Audio: Prioridades y Cooldowns\n",
    "> **Problema:** La \"fatiga de alertas\" ocurre cuando el sistema repite el mismo mensaje constantemente (ej: \"Coche detectado\") impidiendo escuchar otros avisos importantes.\n",
    "> **Soluci√≥n:** Sistema de cola de audio inteligente.\n",
    "*   **Prioridades:** Se asigna un nivel de importancia a cada evento (1: Peligro Inminente, 2: Precauci√≥n, 3: Info). Si ocurren dos eventos a la vez, solo suena el m√°s cr√≠tico.\n",
    "*   **Cooldowns Individuales:** Cada tipo de alerta tiene su propio temporizador. Si suena \"Coche\", ese mensaje espec√≠fico se silencia por 10s, pero el canal queda libre para \"Sem√°foro Rojo\" inmediatamente.\n",
    "*   **Impacto:** Reduce el estr√©s del usuario y garantiza que la informaci√≥n nueva y cr√≠tica siempre tenga preferencia sobre la repetici√≥n de informaci√≥n conocida.\n",
    "\n",
    "### 8. üëÅÔ∏è Accesibilidad Visual Mejorada\n",
    "> **Problema:** Los elementos gr√°ficos est√°ndar (bounding boxes de 2px, textos peque√±os) son dif√≠ciles de percibir para usuarios con baja visi√≥n.\n",
    "> **Soluci√≥n:** Escalado agresivo de elementos de interfaz.\n",
    "*   **Mecanismo:** Aumento del grosor de l√≠neas a 8-10px, tama√±o de fuente a escala 3.0 y radar ampliado a 400px.\n",
    "*   **Impacto:** Garantiza que las alertas visuales sean perceptibles incluso con agudeza visual reducida, priorizando la legibilidad sobre la est√©tica convencional.\n",
    "\n",
    "### 9. üì° Radar de Seguridad Permanente\n",
    "> **Problema:** Al filtrar por modo (ej: \"Solo sem√°foros\"), el usuario pierde consciencia situacional de veh√≠culos cercanos que podr√≠an ser peligrosos.\n",
    "> **Soluci√≥n:** Radar siempre activo para veh√≠culos.\n",
    "*   **Mecanismo:** Los veh√≠culos (coches, motos, buses, camiones) **siempre** aparecen en el radar independientemente del modo seleccionado. El resto de objetos solo aparecen si son relevantes para el modo actual.\n",
    "*   **Impacto:** Mantiene la seguridad cr√≠tica del usuario sin saturar la visualizaci√≥n principal, permitiendo foco en la tarea solicitada sin perder alertas de proximidad vehicular.\n",
    "\n",
    "### 10. üß† Consideraciones √âticas y de Seguridad\n",
    "> **Limitaci√≥n:** SafeStep no sustituye la percepci√≥n humana ni garantiza decisiones seguras por s√≠ solo. Es decir, el sistema es asistivo, por lo que el usuario, como se ha visto, no debe depositar confianza plena en el buen funcionamiento del SafeStep, ya que puede haber fallos de percepci√≥n, latencias o errores de clasificaci√≥n.\n",
    "\n",
    "### 11. üåç Dependencia del contexto urbano\n",
    "> **Problema:** A pesar de que el SafeStep cubre la mayor√≠a de los contextos y generaliza bastante bien, puede ser que en entornos urbanos externos la normativa visual sea distinta y se haga uso de una se√±alizaci√≥n no convencional, de manera que el sistema no funcionar√≠a con tanto √©xito como en zonas/pa√≠ses est√°ndar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efc4d6",
   "metadata": {},
   "source": [
    "## 6. üìä Evaluaci√≥n final del sistema\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "- **SafeStep** confirma el enorme potencial de los sistemas de asistencia peatonal basados en **Visi√≥n Artificial, Realidad Aumentada y procesamiento multimodal** como herramientas de apoyo real para personas con visi√≥n reducida. El sistema logra integrar de forma eficaz detecci√≥n de objetos, OCR, an√°lisis de movimiento, estimaci√≥n de distancia, comprensi√≥n del lenguaje natural y generaci√≥n de alertas visuales y sonoras, manteniendo un equilibrio s√≥lido entre **seguridad, accesibilidad y rendimiento en tiempo real**.\n",
    "\n",
    "- Las decisiones de dise√±o adoptadas a lo largo del proyecto ‚Äîcomo el **OCR selectivo**, el **filtrado por intenci√≥n del usuario**, la **gesti√≥n inteligente de prioridades y cooldowns de audio**, y la **optimizaci√≥n mediante salto de frames**‚Äî han resultado fundamentales para evitar tanto la sobrecarga computacional como la saturaci√≥n cognitiva del usuario, uno de los principales desaf√≠os en entornos urbanos complejos. \n",
    "\n",
    "- Si bien el sistema presenta limitaciones inherentes al uso de visi√≥n monocular y a la variabilidad del entorno real (falsos positivos, errores de OCR en v√≠deo, ambig√ºedades crom√°ticas o sensibilidad al movimiento de c√°mara), estas han sido **claramente identificadas, justificadas y contextualizadas** dentro del alcance de un prototipo funcional. En conjunto, **SafeStep** no solo valida la viabilidad t√©cnica del enfoque propuesto, sino que establece una **base s√≥lida, modular y extensible** para futuras mejoras, como modelos especializados, fusi√≥n de sensores o adaptaci√≥n din√°mica al contexto, consolid√°ndose como una **prueba de concepto robusta, realista y centrada en la accesibilidad urbana**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
